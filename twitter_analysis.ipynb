{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Twitter analysis\n",
    "- scrape twitter by keyword and city location\n",
    "- clean text: remove URLs, and words starting with @ and #\n",
    "- save to csv file:  keyword_city.csv\n",
    "- run sentiment analysis over all city files for a common key word (keyword_*.csv)\n",
    "- after some research: there is no meaningful way to make this into a single value, so just keep all 3 results\n",
    "- store results in a dataframe with cities as index and 3 columns: positive, neutral, negative \n",
    "- see https://pythontic.com/pandas/dataframe-plotting/bar%20chart stacked bar chart for an example\n",
    "- visualize dataframe as barchart with 3 stacked values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import time\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import pytz\n",
    "import re\n",
    "\n",
    "def clean(text):\n",
    "    \n",
    "    text = re.sub(r\"@\\w*\", \"\", text) # remove all words starting with @\n",
    "    text = re.sub(r\"#\\w*\", \"\", text)\n",
    "    text = re.sub(r\"http\\S*\", \"\", text) \n",
    "    text = text.encode('ascii', 'ignore').decode('UTF-8')  #removes unicode chars like emoticons\n",
    "    return text\n",
    "\n",
    "def tweets_to_csv(keyword, city):\n",
    "\n",
    "    client = tweepy.Client(\"your key"),
    "\n",
    "    # make query: https://developer.twitter.com/en/docs/twitter-api/tweets/search/integrate/build-a-query\n",
    "    #query = f'\"{keyword}\" has:geo place:\"{city}\" lang:en -is:retweet'   # Not sure if has:geo is needed as we use place already?\n",
    "    query = f'\"{keyword}\" place:\"{city}\" lang:en -is:retweet' \n",
    "\n",
    "\n",
    "    # set timeframe for query\n",
    "    end_time = datetime.now()\n",
    "    end_time = pytz.UTC.localize(end_time) # add timezone to make it RFC 3339 compliant\n",
    "\n",
    "    start_time = datetime(2015, 7, 1, 0, 0, 0, 0) # Go back until Jan 1, 2015\n",
    "    start_time = pytz.UTC.localize(start_time)\n",
    "\n",
    "    n = 0 # running total number for scraped tweets\n",
    "\n",
    "    # data table, will later be converted into a dataframe\n",
    "    data = {\n",
    "        \"created_at\":[],\n",
    "        #\"author_id\":[],\n",
    "        #\"geo\":[],\n",
    "        \"text\":[],\n",
    "    }\n",
    "\n",
    "    # Loops through the given time frame.\n",
    "    while end_time > start_time: \n",
    "        tweets = tweepy.Paginator(client.search_all_tweets, \n",
    "                            query = query,\n",
    "                            tweet_fields = ['text',\"created_at\", \"author_id\", \"geo\"],\n",
    "                            expansions = 'author_id',\n",
    "                            #start_time = '2021-01-20T00:00:00Z',\n",
    "                            #end_time = '2022-01-21T00:00:00Z',\n",
    "                            start_time = start_time,\n",
    "                            end_time = end_time, \n",
    "                            max_results=500).flatten(limit=500)\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "        got_any_tweets = False\n",
    "\n",
    "        # go through all tweets we got so far\n",
    "        for i, tweet in enumerate(tweets):      \n",
    "            got_any_tweets = True\n",
    "\n",
    "            text = clean(tweet[\"text\"]) # removes URLs, @ and #\n",
    "\n",
    "            date = tweet[\"created_at\"]\n",
    "            print(n+i, date, text[:100])\n",
    "\n",
    "           \n",
    "            data[\"created_at\"].append(tweet[\"created_at\"])\n",
    "            data[\"text\"].append(text) # Store cleaned text\n",
    "        \n",
    "        n += i\n",
    "        end_time = date\n",
    "        time.sleep(1)\n",
    "\n",
    "        if got_any_tweets == False:\n",
    "            print(\"No more tweets, done!\")\n",
    "            break\n",
    "\n",
    "\n",
    "    # convert to dataframe and save as csv\n",
    "    # doing this after each block so we have something in case we get a 429 error \n",
    "   \n",
    "    df = pd.DataFrame.from_dict(data)\n",
    "    file_name = keyword + \"_\" + city + \".csv\"\n",
    "    df.to_csv(file_name, header=True, index=False)\n",
    "    print(\"Saved\", n, \"tweets in\", file_name, \"from now, ending at\", date)\n",
    "    return df\n",
    "    \n",
    "\n",
    "df = tweets_to_csv(\"Gun Control\", \"Houston\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run with a batch on cities\n",
    "cities = [\"Houston\", \"New York\", \"San Francisco\"] # Add more!\n",
    "keyword = \"Gun Control\"\n",
    "\n",
    "for city in cities:\n",
    "    tweets_to_csv(keyword, city)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# This sets up the analysis. Only run this once as it downloads the model\n",
    "# Note: I didn't get this to work b/c my PC wasn't set up for running tensorflow\n",
    "# You'll need to describe what needs to be done to set this up in your user's guide\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from scipy.special import softmax\n",
    "\n",
    "\n",
    "MODEL = f\"cardiffnlp/twitter-roberta-base-sentiment\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I made this a function that should work (i.e. I didn't use a class, which may be wrong ...)\n",
    "\n",
    "\n",
    "def run_sentiment_analyzer(data, tokenizer, model):\n",
    "    # Run for Roberta Model\n",
    "    encoded_text = tokenizer(data, return_tensors='pt')\n",
    "    output = model(**encoded_text)\n",
    "    scores = output[0][0].detach().numpy()\n",
    "    scores = softmax(scores)\n",
    "    scores_dict = {\n",
    "        'roberta_neg' : scores[0],\n",
    "        'roberta_neu' : scores[1],\n",
    "        'roberta_pos' : scores[2]\n",
    "    }\n",
    "    return scores_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run analysis on dataframe we got back earlier\n",
    "print(run_sentiment_analyzer(df[\"text\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run analysis on a batch of cities\n",
    "# do this after you've created several csv files\n",
    "from glob import glob\n",
    "\n",
    "keyword = \"Gun Control\"\n",
    "\n",
    "# columns for making the data frame later, start with empty list = no rows\n",
    "data = {\"neg\":[], \"neu\":[], \"pos\":[]}\n",
    "\n",
    "for city in cities:\n",
    "    filename = keyword + \"_\" + city + \".csv\"\n",
    "    df = pd.read_csv(filename)\n",
    "    res_dict = run_sentiment_analyzer(df[\"text\"])\n",
    "    \n",
    "    # append a row of sentiment values to the columns dict \n",
    "    data[\"neg\"].append(res_dict[\"roberta_neg\"])\n",
    "    data[\"neu\"].append(res_dict[\"roberta_neu\"])\n",
    "    data[\"pos\"].append(res_dict[\"roberta_pos\"])\n",
    "    \n",
    "\n",
    "df = pd.DataFrame(data=data, index=cities)\n",
    "print(df)  # you could save this as csv ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot as stacked bar chart\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1813b1874f6f84418af257d52e08e51b77c1fa4a78b2eeb48d9f803ad6bb1049"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
